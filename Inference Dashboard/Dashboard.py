import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import pickle
from streamlit_echarts import st_echarts
import time
from datetime import datetime
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="íƒœì–‘ê´‘ ì˜ˆì¸¡ ëª¨ë¸", layout="wide")

# CSS ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
    <style>
    .main {
        padding: 2rem;
    }
    .stMetric {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .info-box {
        background-color: #e6f3ff;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    </style>
""", unsafe_allow_html=True)

# ì œëª©
st.title("â˜€ï¸ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡")

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()
start_datetime = datetime.now()

# ë°ì´í„° ì—…ë¡œë“œ
st.sidebar.header("ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["csv"])

features = ['Ampe_R', 'Ampe_S','Ampe_T','ì¼ì‚¬(MJ/m2)', 'ê¸°ì˜¨(Â°C)']
iqr_features = ['Ampe_R', 'Ampe_S','Ampe_T','ì¼ì‚¬', 'ê¸°ì˜¨']

# output_dir ìƒì„±
output_dir = './output'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

def data_eda(df,l,u):
    df['ymdhms'] = pd.to_datetime(df['ymdhms'])
    threshold = 30  # ì—°ì†ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” ìµœì†Œ íšŸìˆ˜
    is_zero = df['generate_gap'] == 0  # ê°’ì´ 0ì¸ ìœ„ì¹˜ë¥¼ Trueë¡œ í‘œì‹œ
    zero_group = is_zero.astype(int).groupby(is_zero.ne(is_zero.shift()).cumsum()).cumsum()  # ì—°ì† ì¹´ìš´íŠ¸
    df['Hour'] = df.ymdhms.dt.hour

    # ì¡°ê±´ ë§Œì¡±í•˜ëŠ” êµ¬ê°„ ì¶”ì¶œ
    long_zero_indices = df.index[zero_group >= threshold]
    df = df.drop(long_zero_indices,axis=0).reset_index(drop=True)
    df.loc[df.Hour == 0 , 'generate_gap'] = 0
    
    #df_filled = df.fillna(method='bfill')
    
    return df

if uploaded_file is not None:
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(uploaded_file,encoding='cp949')
    # ì—…ë¡œë“œëœ íŒŒì¼ ì´ë¦„ ì €ì¥
    uploaded_filename = uploaded_file.name
    base_filename = os.path.splitext(uploaded_filename)[0]  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ì„¹ì…˜
    st.markdown("### ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    st.markdown("#### ë°ì´í„° ê¸°ë³¸ ì •ë³´")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê¸°ê°„", f"{df['ymdhms'].iloc[0]} ~ {df['ymdhms'].iloc[-1]}")
    with col2:
        st.metric("í–‰ ìˆ˜", f"{len(df):,}í–‰")
    with col3:
        st.metric("ì—´ ìˆ˜", f"{len(df.columns):,}ì—´")
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
    st.dataframe(df, use_container_width=True)

    # íŠ¹ì§• ë° íƒ€ê²Ÿ ì„ íƒ
    st.sidebar.subheader("íŠ¹ì§• ë° íƒ€ê²Ÿ ì„ íƒ")
    target_col = 'generate_gap'
    ids =  st.sidebar.selectbox("ID ì„ íƒ", df.id.unique())
    

    if target_col and ids:
        iqr_df = pd.read_csv('./IQR_DATA/iqr_df.csv')
        l = iqr_df.iloc[ids-1]['lower']
        u = iqr_df.iloc[ids-1]['upper']

        data = data_eda(df,l,u)  

        # ê° featureì— ëŒ€í•´ ì´ìƒì¹˜ ì²˜ë¦¬
        for feature,iqr_features in zip(features,iqr_features):
            feature_iqr = pd.read_csv(f'./IQR_DATA/{iqr_features}.csv')
            mi = feature_iqr.iloc[ids-1]['Min']
            ma = feature_iqr.iloc[ids-1]['Max']
            avg = feature_iqr.iloc[ids-1]['Avg']

            feature_values = data[feature]
            
            # ê²°ì¸¡ê°’ í™•ì¸ ë° ì²˜ë¦¬
            missing_mask = feature_values.isnull()
            if missing_mask.sum() > 0:
                # ê²°ì¸¡ê°’ ë°œìƒ ì •ë³´ ì¶œë ¥
                missing_indices = data.index[missing_mask]
                missing_times = data.loc[missing_mask, 'ymdhms']
                
                st.markdown(f"### âš ï¸ {feature} ê²°ì¸¡ê°’ ê°ì§€")
                st.markdown(f"<div class='info-box'>", unsafe_allow_html=True)
                st.markdown(f"- ë°œê²¬ëœ ê²°ì¸¡ê°’ ê°œìˆ˜: **{len(missing_indices):,}ê°œ**")
                
                # ê²°ì¸¡ê°’ ìƒì„¸ ì •ë³´ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
                missing_df = pd.DataFrame({
                    'ì‹œê°„': missing_times,
                    'ëŒ€ì²´ë  í‰ê· ê°’': avg
                })
                st.dataframe(missing_df, use_container_width=True)
                st.markdown("</div>", unsafe_allow_html=True)
                
                # ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
                data.loc[missing_mask, feature] = avg
            
            # ì´ìƒì¹˜ ì²˜ë¦¬
            outlier_mask = (feature_values < mi) | (feature_values > ma)
            if outlier_mask.sum() > 0:
                # ì´ìƒì¹˜ ë°œìƒ ì •ë³´ ì¶œë ¥
                outlier_indices = data.index[outlier_mask]
                outlier_values = data.loc[outlier_mask, feature]
                outlier_times = data.loc[outlier_mask, 'ymdhms']
                
                st.markdown(f"### âš ï¸ ì´ìƒì¹˜ ê°ì§€ : {feature}")
                st.markdown(f"- ë°œê²¬ëœ ì´ìƒì¹˜ ê°œìˆ˜: **{len(outlier_indices):,}ê°œ**")
                
                # ì´ìƒì¹˜ ìƒì„¸ ì •ë³´ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
                outlier_df = pd.DataFrame({
                    'ì‹œê°„': outlier_times,
                    'ì´ìƒì¹˜ ê°’': outlier_values,
                    'í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´': avg
                })
                st.dataframe(outlier_df, use_container_width=True)
                st.markdown("</div>", unsafe_allow_html=True)
                
                data.loc[outlier_mask, feature] = avg
    

        zero_index = data.loc[data['generate_gap'] <= 0].index
        value_index = data.loc[data['generate_gap'] > 0].index  

        X_test = data.iloc[value_index][features]
        y_test = data.iloc[value_index][target_col]
        
        fea_sc_path = './feature_scaler/feature_scaler_' + str(ids)+ '.pkl'
        with open(fea_sc_path, 'rb') as file:
            feature_scaler = pickle.load(file)

        tar_sc_path = './target_scaler/target_scaler_' + str(ids)+ '.pkl'
        with open(tar_sc_path, 'rb') as file:
            target_scaler = pickle.load(file)
        
        #ì •ê·œí™”
        Scaled_X_test= feature_scaler.transform(X_test)

        Test_df = pd.DataFrame(Scaled_X_test, columns=features)
        Test_df['ds'] = data.iloc[value_index]['ymdhms'].values

        # ëª¨ë¸ ë¡œë“œ 
        model_path_ = './prophet/solar_model' + str(ids)+ '.pkl'
        with open(model_path_, "rb") as f:
            loaded_model = pickle.load(f)

        # ì˜ˆì¸¡
        forecast = loaded_model.predict(Test_df)
        forecast['yhat'] = forecast['yhat'].apply(lambda x: max(x, 0))

        # ì—­ë³€í™˜
        y_pred_sc_bi = target_scaler.inverse_transform(pd.DataFrame(forecast['yhat'].values, index=value_index))

        predict_result = pd.DataFrame(index = range(0,len(data)),columns=['ymdhms','GT','Pred'])
        predict_result['ymdhms'] = data['ymdhms']  
        predict_result['ymdhms'] = data['ymdhms'].dt.strftime("%Y-%m-%d %H:00")       

        predict_result.iloc[value_index, predict_result.columns.get_loc('GT')] = list(y_test)
        predict_result.iloc[value_index, predict_result.columns.get_loc('Pred')] = list(y_pred_sc_bi.squeeze())

        predict_result.iloc[zero_index, predict_result.columns.get_loc('GT')] = 0 
        predict_result.iloc[zero_index, predict_result.columns.get_loc('Pred')] = 0 

        # ê²°ê³¼ ì €ì¥
        # output_filename = f'{base_filename}_pred.csv'
        # output_path = os.path.join(output_dir, output_filename)
        # predict_result.to_csv(output_path, index=False, encoding='cp949')
        # st.success(f"ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_filename}")

        # ì„±ëŠ¥ í‰ê°€
        MAPE = np.mean(np.abs((predict_result.iloc[value_index]['GT'] - predict_result.iloc[value_index]['Pred']) /  predict_result.iloc[value_index]['GT'])) * 100
        
        # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        end_time = time.time()
        end_datetime = datetime.now()
        execution_time = end_time - start_time
        
        # ê²°ê³¼ ì¶œë ¥ 
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥")
            st.metric("MAPE", f"{MAPE:.2f}%")
        with col2:
            st.markdown("### â±ï¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì •ë³´")
            st.metric("ì‹¤í–‰ ì‹œê°„", f"{execution_time:.2f}ì´ˆ")
        

        # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
        st.markdown("### ğŸ“‰ ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ê·¸ë˜í”„")
        st.markdown("#### ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼")

        # Convert datetime to string format
        x_axis_data = [str(dt) for dt in predict_result['ymdhms'].values]

        option = {
            "tooltip": {"trigger": "axis"},
            "legend": {"data": ["Actual", "Predicted"]},
            "xAxis": {
                "type": "category",
                "data": x_axis_data,
            },
            "yAxis": {"type": "value"},
            "series": [
                {
                    "name": "Actual",
                    "type": "line",
                    "data": list(predict_result['GT'].values),
                    "smooth": True,
                },
                {
                    "name": "Predicted",
                    "type": "line",
                    "data": list(predict_result['Pred'].values),
                    "smooth": True,
                },
            ],
        }

        # ECharts ì°¨íŠ¸ ì¶œë ¥
        st_echarts(options=option, height="500px")

    # ê²°ê³¼ ì €ì¥
        output_filename = f'{base_filename}_pred.csv'
        output_path = os.path.join(output_dir, output_filename)
        predict_result.to_csv(output_path, index=False, encoding='cp949')
        st.success(f"ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_filename}")

else:
    st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

